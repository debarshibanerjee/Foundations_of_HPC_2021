mpi_examples:

MPI is "shared nothing approach"

A. basic-mpi-codes

1. hello_world:

	mpicc hello_world.c -o hello_world.x
	mpirun -np 4 hello_world.x
	mpirun -np 4 hello_world.x 2> /dev/null		# for suppressing errors
	mpirun -np 4 hello_world.x 2> /dev/null | sort	# sort them if you want just to verify if everything is fine, but by default the order of printing it is random.

	Fortran version:
	mpif90 hello_world.F90 -o hello_world_fortran.x
	mpirun -np 4 hello_world_fortran.x

	the original Fortran version had significant errors.
	I fixed them so that it works.

	prints output in random order depending on how/when threads are initialised.

2. mpi_pi.c:

	mpicc mpi_pi.c -o mpi_pi.x

	we need to mention number of iterations when executing this -
	mpirun -np 4 mpi_pi.x 10000000

	Compiled 2 diff versions with Send and Ssend. Run them as -
	mpirun -np 4 mpi_pi_Ssend.x 10000000
	mpirun -np 4 mpi_pi_Send.x 10000000

	we read in the number of iterations in a 'long long int' to be able to have very large values.
	MPI_Wtime() gives us the time at any given point.
	We use some monte-carlo integration in this code.
	google later to see how that works.

	at the end, we collect data on one processor, here, '0'.
	so the master processor gathers results from all other processors, computes pi, and then prints the results.
	the others just send data to the master.

	we change Ssend (synchronous send - completes when receive is complete) to Send and see how things work.

	Send is not necessarily blocking here.
	This is because when we comment out the end of the code and see the
	communication time taken in total, it is a very tiny fraction of overall time.
	So here Send() just uses the local buffer and things aren't really blocking.

	So, data is copied to a buffer by Send() and it is non-blocking.
	For Ssend() we need confirmation of receiving, but even there the receive part is non-blocking.
	This means that the received data is received in a buffer and before it is copied to final location,
	the sending processor is told that the Ssend() is complete. Hence, even that is not really ordered.

	We have the wait-time (time to sleep) commands where we force each processor to wait a certain time,
	and that effectively serializes the operation. This leads to ordered output / sorted output.

	In this, when we run using "-np 8", for Ssend() we can clearly see the output come one by one (0 at the end).
	Same for Send() with the small difference that last one (7) and '0' can get interchanged during printing.
	Probably this is due to the fact that in Send() the receive confirmation is not needed, and for the last one,
	since right after it '0' prints, there is so little time difference that either can be printed first/second.
	But for Ssend() since the receive confirmation is needed, even the last one seems to be always ordered.

3. send_message.F90:

	mpif90 send_message.F90 -o send_message.x
	mpirun -np 4 send_message.x

	Here, we send a simple message from processor 0 to processor 1.
	No matter how many processors we run this code with we only use those 2 processors.
	Finally the receiving processor prints the received value and confirms that simple sending operation worked.

4. deadlock.c:

	mpicc deadlock.c -o deadlock.x
	mpicc deadlock.c -o deadlock_resolved.x
	mpirun -np 2 deadlock.x
	mpirun -np 2 deadlock_resolved.x

	we set diff values of source, destination, tags etc for 2 processors.
	but this code is deadlocked since both processors will execute the Send() first and that will never
	complete since neither processor is receiving the other's sent message.

	we can also see the deadlock using Ssend() instead of Send()

	the commented out section at the end resolves the deadlock.
	it ensures that one processor sends first and the other receives first.
	hence, there is no deadlock.

5. linear-array.c:

	mpicc linear-array.c -o linear-array.x
	mpirun -np 4 linear-array.x

	the "master" processor (0), sends a message=0 to the next one.
	every other processor (except for the last one), adds its rank to that message.
	this creates a linear array of processors (sort of).
	the last one adds its rank, and then prints the final sum - which should be = n*(n+1)/2 [n = no. of processors used]

6. mpi_env_call.c:

	mpicc mpi_env_call.c -o mpi_env_call.x
	mpirun -np 4 mpi_env_call.x

	this prints "the name of the processor" MPI processes are running on.
	for my laptop it just prints the hostname (mintbox).
	same for orfeo...it prints the name of the host/node [ct1pt-tnode008 - example]



B. collective-mpi

1. bcast.f:

	mpif77 bcast.f -o bcast.x
	mpirun -np 4 bcast.x

	sends (broadcasts) the array "a" to all processors and then prints the data stored there.
	we can contrast output before (all 0,0) and after broadcast (all 2,4) just to be sure.

2. b_cast.c:

	mpicc b_cast.c -o b_cast.x
	mpirun -np 4 b_cast.x

	I changed the code to make behaviour similar to the FORTRAN version.
	Original one didn't even print anything.

3. gather.f:

	mpif77 gather.f -o gather.x
	mpirun -np 8 --oversubscribe gather.x
	(only runs on 8 cpus properly)

	we gather 2 elements from each CPU in an array of 16 elements, where each element = rank of that cpu.
	then we print the "gathered array" which is 0,0,1,1,2,2....,7,7

4. scatter.f:

	mpif77 scatter.f -o scatter.x
	mpirun -np 8 --oversubscribe scatter.x

	we scatter an array of 16 elements [where each element = i, i==index of array] to 8 processors.
	each processor prints out 2 elements at a time with its rank.
	although order is random, the array is scattered consecutively amongst the processors.
	that is proc 0 - 1,2
			proc 1 - 3,4
			proc 2 - 5,6
			...
			etc.

	if we run with more than 16 processors, from processor 9 (rank 8) onwards we get garbage data since
	it receives some data at some memory address which is invalid (unallocated).
	for gather.f, this invalid memory reference causes segfaults.

5. reduce.f:

	mpif77 reduce.f -o reduce.x
	mpirun -np 4 reduce.x

	this can be run on any number of processors.

	here we have each processor define the 2 elements (a(1),a(2)) as 2, 4 respectively.
	then we reduce it.
	so, a(1)=2*num_of_processors
	so, a(2)=4*num_of_processors

	so reduce sums (since we choose MPI_SUM here) each corresponding element of the array in each processor.

6. mpi_bcastcompare.c:

	mpicc mpi_bcastcompare.c -o mpi_bcastcompare.x
	mpirun -np 4 mpi_bcastcompare.x

	here, our own broadcast simply sends data to all processes (remember to not send to self), if it is root.
	and it receives if it is any other process.

	we compare this with MPI's own broadcast implementation.

	for small sizes, our one performs better (by tiny margins). for larger sizes, we get significant difference using MPI_Bcast().

	run on orfeo with --oversubscribe and increasing no. of processors on a computational node:
	increase the following:
	num_elements = 1000; num_trials = 100;
	-np 128 shows factor of 10 difference between our own broadcast and MPI's own implementation of the same.



C. virtual-topology

1. 2D-thorus.c:

	mpicc 2D-thorus.c -o 2D-thorus.x
	mpirun -np 12 -oversubscribe 2D-thorus.x

	we let MPI choose appropriate dims (hence [0,0]) for the cartesian grid.
	since it is a thorus, both dimensions are made periodic.
	reorder=true is set in case MPI does something better on our physical topology.
	then we create the new communicator in cartesian grid, and get new ranks and coordinates.
	this way we see the 2D thorus take shape.

	change: dims[2] = {0, 0}
	to:	dims[2] = {1, 0} OR {2,0} OR {change 2nd coordinate instead of first} etc.

	in all of these we create some restrictions on dimensions MPI can choose since it will
	not overwrite the non-zero dimension.

	if we run with '7' processors, for example, MPI chooses a 7x1 grid since nothing else works.

2. cart_coords.c:

	mpicc cart_coords.c -o cart_coords.x
	mpirun -np 12 --oversubscribe cart_coords.x

	only works with 12 processors

	creates a 4x3 grid with periodicity along X axis, but not Y.
	sort of a cylindrical shape.
	then we use MPI_Cart_rank to print rank of a processor with given coordinates.
	we also do the reverse with MPI_Cart_coords.

3. cart_shift.c:

	mpicc cart_shift.c -o cart_shift.x
	mpirun -np 12 --oversubscribe cart_shift.x

	we build a 4x3 cartesian grid, periodic along both dimensions (2D thorus).
	then we use Cart_shift along both X[0] and Y[1] dimensions to displace by 1 element each.
	then we print the neighbours of each processor in the original grid with the new
	neighbour values that we have after 1 displacement in both directions.

4. ring-array.c:

	mpicc ring-array.c -o ring-array.x
	mpirun -np 4 ring-array.x

	we make each processor send a value(=20) in ring topology.
	each processor sends the message to it's rank+1.
	finally, rank=numproc-1 sends its message to rank=0.

5. ring-non-blocking.c:

	mpicc ring-non-blocking.c -o ring-non-blocking.x
	mpirun -np 4 ring-non-blocking.x

	it's a template code which shows use of non-blocking Isend and Irecv in a ring topology.
	it compiles but there's nothing to do in it.
	it can however be used for other such processes.

6. ring-virtual.c:

	mpicc ring-virtual.c -o ring-virtual.x
	mpirun -np 4 ring-virtual.x

	here we create a ring using cartesian topology.
	a 1-d cartesian grid (a line), with periodicity, is a ring.
	then we input a number at proc=0, and send it to the next proc.
	the next proc(=1) receives the message and passes it to the next one and so on.
	since proc=0 has no receive, so the ring terminates after one iteration is complete.

	but then the loop asks for a new value again and repeats until it gets a -ve value then it terminates.
	slightly modified from the original by me for sake of convenience.

